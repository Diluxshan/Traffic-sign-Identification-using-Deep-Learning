# -*- coding: utf-8 -*-
"""Model_loss_RGB_8_04052020_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RmuXrgb7DWlihKaUC9gvlEkOgG5EIj8I
"""

import tensorflow as tf
import tensorflow_hub as hub

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
import os
import numpy as np
import matplotlib.pyplot as plt
from time import gmtime, strftime

#from google.colab import drive
#drive.mount('/content/drive')

PATH = '/content/drive/My Drive/images/'     # same path put in create_img_folder.py 
model_name = "efficientnet_V1"
batch_size = 64
epochs = 10
IMG_HEIGHT = 224
IMG_WIDTH = 224



#zip_file=tf.keras.utils.get_file(origin='https://storage.googleapis.com/plantdata/PlantVillage.zip', fname='PlantVillage.zip', extract=True)
PATH_IMAGE = str(PATH)+'images_folder/'

TRAIN_PATH = str(PATH_IMAGE)+'train_dir'
VAL_PATH = str(PATH_IMAGE)+'val_dir'

train_dir = os.path.join(PATH_IMAGE, 'train_dir')
validation_dir = os.path.join(PATH_IMAGE, 'val_dir')
total_train = 0
total_val = 0
num_tr = np.zeros((102,), dtype=int)
num_val = np.zeros((102,), dtype=int)
num = 0
for dir_path in range(102):   # directory with our training pictures
  DIR_tr = os.path.join(TRAIN_PATH, str(dir_path)) 
  total_train += len(os.listdir(DIR_tr))
  num_tr[num] = len(os.listdir(DIR_tr))
  DIR_val = os.path.join(VAL_PATH, str(dir_path))
  total_val += len(os.listdir(DIR_val))
  num_val[num] = len(os.listdir(DIR_val))
  num+=1
  
#for x in range(num):
   #print('total training {} images:'.format(dir[x]), num_tr[x])
#for x in range(num):
   #print('total validation {} images:'.format(dir[x]), num_val[x]) 

print("--")
print("Total training images:", total_train)
print("Total validation images:", total_val)

Do_Data_augmentation = True
if Do_Data_augmentation:
	train_image_generator  = ImageDataGenerator(rescale=1./255,
																							rotation_range=40,
																							width_shift_range=.15,
																							height_shift_range=.15,
																							horizontal_flip=True,
																							zoom_range=0.2,
																							shear_range=0.2                     
																							)
 
else:
	train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data

validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,
                                                           batch_size = batch_size,                                                        
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           subset="training", 
                                                           shuffle=True, 
                                                           seed=42,
                                                           color_mode="rgb",
                                                           class_mode="categorical"                                                          
                                                           )



val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,
                                                              batch_size = batch_size,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              shuffle=False, 
                                                              seed=42,
                                                              color_mode="rgb",
                                                              class_mode="categorical"
                                                              )

for image_batch, label_batch in train_data_gen:
  print("Image batch shape: ", image_batch.shape)
  print("Label batch shape: ", label_batch.shape)
  break



def get_model_path(model_name):
  if model_name == "Mobilenet":
    feature_extractor_url = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2" 
  elif model_name == "ResNet_50":
    feature_extractor_url = "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4"
  elif model_name == "efficientnet":
    feature_extractor_url = "https://tfhub.dev/google/efficientnet/b7/feature-vector/1"
  else:
    print("Recheck the model name!!!")
  return feature_extractor_url

feature_extractor_url = get_model_path("efficientnet")

feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(IMG_HEIGHT,IMG_WIDTH,3)
                                         )      #output_shape=[102]   # Outputs a tensor with shape [batch_size, 102].
feature_batch = feature_extractor_layer(image_batch)
print(feature_batch.shape)

Do_fine_tuning = False
if Do_fine_tuning:
  feature_extractor_layer.trainable = True
  # unfreeze some layers of base network for fine-tuning
  for layer in feature_extractor_layer.layers[-30:]:
      layer.trainable =True

else:
  feature_extractor_layer.trainable = False

model = tf.keras.Sequential([feature_extractor_layer,
                             tf.keras.layers.Flatten(),
                             tf.keras.layers.Dense(512, activation='relu'),
                             tf.keras.layers.Dropout(rate=0.2),
                             tf.keras.layers.Dense(train_data_gen.num_classes, activation='softmax',
                                                   kernel_regularizer=tf.keras.regularizers.l2(0.0001))
                             ])

model.summary()


#efficientnet

weight_for_ = np.zeros((102,), dtype=float)
for i in range(102):
  weight_for_[i] = (1 / num_tr[i])*(total_train)/102.  


class_weight = {0: weight_for_[0], 1: weight_for_[1], 2: weight_for_[2], 3: weight_for_[3], 4: weight_for_[4], 5: weight_for_[5], 6: weight_for_[6], 7: weight_for_[7], 8: weight_for_[8], 9: weight_for_[9], 10: weight_for_[10], 11: weight_for_[11], 12: weight_for_[12], 13: weight_for_[13], 14: weight_for_[14], 15: weight_for_[15], 16: weight_for_[16], 17: weight_for_[17], 18: weight_for_[18], 19: weight_for_[19], 20: weight_for_[20], 21: weight_for_[21], 22: weight_for_[22], 23: weight_for_[23], 24: weight_for_[24], 25: weight_for_[25], 26: weight_for_[26], 27: weight_for_[27], 28: weight_for_[28], 29: weight_for_[29], 30: weight_for_[30], 31: weight_for_[31], 32: weight_for_[32], 33: weight_for_[33], 34: weight_for_[34], 35: weight_for_[35], 36: weight_for_[36], 37: weight_for_[37], 38: weight_for_[38], 39: weight_for_[39], 40: weight_for_[40], 41: weight_for_[41], 42: weight_for_[42], 43: weight_for_[43], 44: weight_for_[44], 45: weight_for_[45], 46: weight_for_[46], 47: weight_for_[47], 48: weight_for_[48], 49: weight_for_[49], 50: weight_for_[50], 51: weight_for_[51], 52: weight_for_[52], 53: weight_for_[53], 54: weight_for_[54], 55: weight_for_[55], 56: weight_for_[56], 57: weight_for_[57], 58: weight_for_[58], 59: weight_for_[59], 60: weight_for_[60], 61: weight_for_[61], 62: weight_for_[62], 63: weight_for_[63], 64: weight_for_[64], 65: weight_for_[65], 66: weight_for_[66], 67: weight_for_[67], 68: weight_for_[68], 69: weight_for_[69], 70: weight_for_[70], 71: weight_for_[71], 72: weight_for_[72], 73: weight_for_[73], 74: weight_for_[74], 75: weight_for_[75], 76: weight_for_[76], 77: weight_for_[77], 78: weight_for_[78], 79: weight_for_[79], 80: weight_for_[80], 81: weight_for_[81], 82: weight_for_[82], 83: weight_for_[83], 84: weight_for_[84], 85: weight_for_[85], 86: weight_for_[86], 87: weight_for_[87], 88: weight_for_[88], 89: weight_for_[89], 90: weight_for_[90], 91: weight_for_[91], 92: weight_for_[92], 93: weight_for_[93], 94: weight_for_[94], 95: weight_for_[95], 96: weight_for_[96], 97: weight_for_[97], 98: weight_for_[98], 99: weight_for_[99], 100: weight_for_[100], 101: weight_for_[101]}
print("class_weight:",class_weight)

model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss=tf.keras.losses.CategoricalHinge(),
              metrics=['acc']
              )

#lr=0.001

class CollectBatchStats(tf.keras.callbacks.Callback):
  def __init__(self):
    self.batch_losses = []
    self.batch_acc = []

  def on_train_batch_end(self, batch, logs=None):
    self.batch_losses.append(logs['loss'])
    self.batch_acc.append(logs['acc'])
    self.model.reset_metrics()

# steps_per_epoch = train_data_gen.samples//train_data_gen.batch_size    # result = 410
steps_per_epoch = np.ceil(train_data_gen.samples/train_data_gen.batch_size)   # result = 411
validation_steps = np.ceil(val_data_gen.samples/val_data_gen.batch_size)

batch_stats_callback = CollectBatchStats()

history = model.fit_generator(train_data_gen,
                              epochs=epochs,
                              steps_per_epoch=steps_per_epoch,
                              callbacks = [batch_stats_callback],
                              class_weight = class_weight,
                              validation_data = val_data_gen,
                              validation_steps = validation_steps
                             )


os.environ['TZ'] = 'IST-5:30'
t = strftime("%d_%b_%Y_%H:%M:%S")
export_path = str(PATH_IMAGE)+"{}_{}".format(model_name,t)
model.save(export_path, save_format='tf')

print("export_path:",export_path)

#  !mkdir "tflite_models"
TFLITE_MODEL = str(PATH_IMAGE) +"{}.tflite".format(model_name)


# Get the concrete function from the Keras model.
run_model = tf.function(lambda x : model(x))

# Save the concrete function.
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)
)

# Convert the model to standard TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converted_tflite_model = converter.convert()
open(TFLITE_MODEL, "wb").write(converted_tflite_model)

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(epochs)
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.ylabel("Accuracy (training and validation)")
plt.xlabel("Training Steps")
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.ylabel("Loss (training and validation)")
plt.xlabel("Training Steps")
plt.show()

################################################################33



